{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082e6c34",
   "metadata": {},
   "source": [
    "# TFG: Título del TFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08820e3c",
   "metadata": {},
   "source": [
    "## Hugo López Álvarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a66de8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy    \n",
    "import pandas   \n",
    "import wandb\n",
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss, fbeta_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6b379",
   "metadata": {},
   "source": [
    "## Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e415a",
   "metadata": {},
   "source": [
    "Definición de la clase DatasetTFG que se usará para entrenar al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5917f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTFG(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a33f8e",
   "metadata": {},
   "source": [
    "Definición de la clase Modelo\n",
    "- La capa1 transforma la dimensión de entrada a ventaOculta.Value neuronas\n",
    "- La capa2 pasa de las neuronas de la capa1, a tantas salidas como clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b046d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeloMulticlase(nn.Module):\n",
    "    def __init__(self, input_dim, ventanaOculta, numClases):\n",
    "        super().__init__()\n",
    "        self.capa1 = nn.Linear(input_dim, ventanaOculta)    \n",
    "        self.bn1 = nn.BatchNorm1d(ventanaOculta, momentum=0.01)\n",
    "        self.capa2 =  nn.Linear(ventanaOculta, numClases)\n",
    "        \n",
    "    def forward(self,  X):\n",
    "        X = torch.relu(self.bn1(self.capa1(X)))  \n",
    "        X = self.capa2(X) \n",
    "        return X    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240a0da",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa2cafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_ip_column(df, ip_column_name):\n",
    "    \n",
    "    # Divide la IP en cuatro partes\n",
    "    ip_parts = df[ip_column_name].str.split('.', expand=True)\n",
    "    \n",
    "    # Crea nombres de columnas basados en el nombre original\n",
    "    new_columns = {\n",
    "        0: f\"{ip_column_name}_part1\",\n",
    "        1: f\"{ip_column_name}_part2\", \n",
    "        2: f\"{ip_column_name}_part3\",\n",
    "        3: f\"{ip_column_name}_part4\"\n",
    "    }\n",
    "    \n",
    "    # Se elimina la columna de ip_column_name\n",
    "    df = df.drop(columns=[ip_column_name]) \n",
    "    \n",
    "    # Añade las nuevas columnas al DataFrame\n",
    "    for part, col_name in new_columns.items():\n",
    "        df[col_name] = pandas.to_numeric(ip_parts[part])  # Convierte a numérico\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc3e1",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "281eeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileData = pandas.read_csv('../Datasets/modUQ.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16021721",
   "metadata": {},
   "source": [
    "### Comprobación de la obtención correcta del csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77275e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLOW_START_MILLISECONDS</th>\n",
       "      <th>FLOW_END_MILLISECONDS</th>\n",
       "      <th>IPV4_SRC_ADDR</th>\n",
       "      <th>L4_SRC_PORT</th>\n",
       "      <th>IPV4_DST_ADDR</th>\n",
       "      <th>L4_DST_PORT</th>\n",
       "      <th>PROTOCOL</th>\n",
       "      <th>L7_PROTO</th>\n",
       "      <th>IN_BYTES</th>\n",
       "      <th>IN_PKTS</th>\n",
       "      <th>...</th>\n",
       "      <th>SRC_TO_DST_IAT_MIN</th>\n",
       "      <th>SRC_TO_DST_IAT_MAX</th>\n",
       "      <th>SRC_TO_DST_IAT_AVG</th>\n",
       "      <th>SRC_TO_DST_IAT_STDDEV</th>\n",
       "      <th>DST_TO_SRC_IAT_MIN</th>\n",
       "      <th>DST_TO_SRC_IAT_MAX</th>\n",
       "      <th>DST_TO_SRC_IAT_AVG</th>\n",
       "      <th>DST_TO_SRC_IAT_STDDEV</th>\n",
       "      <th>Label</th>\n",
       "      <th>Attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1424242193040</td>\n",
       "      <td>1424242193043</td>\n",
       "      <td>59.166.0.2</td>\n",
       "      <td>4894</td>\n",
       "      <td>149.171.126.3</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1424242192744</td>\n",
       "      <td>1424242193079</td>\n",
       "      <td>59.166.0.4</td>\n",
       "      <td>52671</td>\n",
       "      <td>149.171.126.6</td>\n",
       "      <td>31992</td>\n",
       "      <td>6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4704</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1424242190649</td>\n",
       "      <td>1424242193109</td>\n",
       "      <td>59.166.0.0</td>\n",
       "      <td>47290</td>\n",
       "      <td>149.171.126.9</td>\n",
       "      <td>6881</td>\n",
       "      <td>6</td>\n",
       "      <td>37.0</td>\n",
       "      <td>13662</td>\n",
       "      <td>238</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1843</td>\n",
       "      <td>10</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>1843</td>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1424242193145</td>\n",
       "      <td>1424242193146</td>\n",
       "      <td>59.166.0.8</td>\n",
       "      <td>43310</td>\n",
       "      <td>149.171.126.7</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1424242193239</td>\n",
       "      <td>1424242193241</td>\n",
       "      <td>59.166.0.1</td>\n",
       "      <td>45870</td>\n",
       "      <td>149.171.126.1</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>130</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FLOW_START_MILLISECONDS  FLOW_END_MILLISECONDS IPV4_SRC_ADDR  L4_SRC_PORT  \\\n",
       "0            1424242193040          1424242193043    59.166.0.2         4894   \n",
       "1            1424242192744          1424242193079    59.166.0.4        52671   \n",
       "2            1424242190649          1424242193109    59.166.0.0        47290   \n",
       "3            1424242193145          1424242193146    59.166.0.8        43310   \n",
       "4            1424242193239          1424242193241    59.166.0.1        45870   \n",
       "\n",
       "   IPV4_DST_ADDR  L4_DST_PORT  PROTOCOL  L7_PROTO  IN_BYTES  IN_PKTS  ...  \\\n",
       "0  149.171.126.3           53        17       5.0       146        2  ...   \n",
       "1  149.171.126.6        31992         6      11.0      4704       28  ...   \n",
       "2  149.171.126.9         6881         6      37.0     13662      238  ...   \n",
       "3  149.171.126.7           53        17       5.0       146        2  ...   \n",
       "4  149.171.126.1           53        17       5.0       130        2  ...   \n",
       "\n",
       "   SRC_TO_DST_IAT_MIN  SRC_TO_DST_IAT_MAX  SRC_TO_DST_IAT_AVG  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                  91                  12   \n",
       "2                   0                1843                  10   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   SRC_TO_DST_IAT_STDDEV  DST_TO_SRC_IAT_MIN  DST_TO_SRC_IAT_MAX  \\\n",
       "0                      0                   0                   0   \n",
       "1                     19                   0                  90   \n",
       "2                    119                   0                1843   \n",
       "3                      0                   0                   0   \n",
       "4                      0                   0                   0   \n",
       "\n",
       "   DST_TO_SRC_IAT_AVG  DST_TO_SRC_IAT_STDDEV  Label  Attack  \n",
       "0                   0                      0      0  Benign  \n",
       "1                  12                     19      0  Benign  \n",
       "2                   5                     88      0  Benign  \n",
       "3                   0                      0      0  Benign  \n",
       "4                   0                      0      0  Benign  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96a971",
   "metadata": {},
   "source": [
    "### Se convierten las columnas no numéricas para poder utilizarlas con pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067862ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoor' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n"
     ]
    }
   ],
   "source": [
    "ataquesData = fileData[fileData['Label'] != 0].copy()\n",
    "ataquesData['Attack'] = LabelEncoder().fit_transform(ataquesData['Attack'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac848f",
   "metadata": {},
   "source": [
    "### Se comprueba que los datos se han transformado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ataquesData.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28c573",
   "metadata": {},
   "source": [
    "## Se eliminan los datos con valores infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"¿Existen valores infinitos en X?: \", numpy.isinf(fileData.values).any())\n",
    "ataquesData = ataquesData.replace([numpy.inf, -numpy.inf], numpy.nan).dropna()\n",
    "#print(\"¿Siguen existiendo valores infinitos en X?: \", numpy.isinf(fileData.values).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1803ca7d",
   "metadata": {},
   "source": [
    "### Se separan las características (X) de la etiqueta (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ataquesData.drop(columns=['Label', 'Attack', 'FLOW_START_MILLISECONDS', 'FLOW_END_MILLISECONDS', 'IPV4_SRC_ADDR', 'IPV4_DST_ADDR']).values\n",
    "Y = ataquesData['Attack'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f7f18",
   "metadata": {},
   "source": [
    "### Se elimina fileData que contiene el csv con los datos para liberar memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del fileData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84d514",
   "metadata": {},
   "source": [
    "## Se separan los datos del entrenamiento de los datos de prueba\n",
    "El entrenamiento tendrá el 80% de los datos\n",
    "\n",
    "La prueba tendrá el 20% de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5706da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entrena, X_prueba, Y_entrana, Y_prueba = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42,  stratify=Y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085b6e7",
   "metadata": {},
   "source": [
    "## Se normalizan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ffa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "escalador = MinMaxScaler(feature_range=(0,1))\n",
    "X_entrena_normalizado = escalador.fit_transform(X_entrena)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a8afc",
   "metadata": {},
   "source": [
    "### Se convierten los datos a tensores de Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entrena_tensor = torch.tensor(X_entrena_normalizado, dtype=torch.float32)\n",
    "Y_entrena_tensor = torch.tensor(Y_entrana, dtype=torch.float32) # Puede que dé problemas con CrossEntropyLoss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fec3c1",
   "metadata": {},
   "source": [
    "## Creación del Dataset personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_entrena = DatasetTFG(X_entrena_tensor, Y_entrena_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49088650",
   "metadata": {},
   "source": [
    "## Se configura pérdida y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frec_clases = numpy.bincount(Y_entrana) # Frecuencia de las clases\n",
    "\n",
    "weight_clases = (len(Y_entrana) - frec_clases)/frec_clases\n",
    "\n",
    "weight_clases_tensor = torch.tensor(weight_clases, dtype=torch.float32)  # Auto-cálculo\n",
    "\n",
    "perdida = nn.CrossEntropyLoss(weight=weight_clases_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4591504a",
   "metadata": {},
   "source": [
    "## Definición de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [32, 64, 128, 256, 512] \n",
    "learning_rate = [1e-2, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5]\n",
    "hidden_factor=[25, 49, 98]\n",
    "epochs= [30, 50, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c5a59",
   "metadata": {},
   "source": [
    "## Se define el objeto KFold que se utilizará para la validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da600f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece36861",
   "metadata": {},
   "source": [
    "## Bucle de entrenamiento o épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134de800",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in batch_size:\n",
    "    for lr in learning_rate:\n",
    "        for hs in hidden_factor:\n",
    "            for e in epochs:\n",
    "                # Listas para almacenar métricas de cada fold\n",
    "                fold_metrics = {\n",
    "                    'loss': [], 'accuracy': [], \n",
    "                    'precision_macro': [], 'recall_macro': [], 'f1_macro': [],\n",
    "                    'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': [],\n",
    "                    'roc_auc_ovo': [], 'roc_auc_ovr': [],\n",
    "                    'confusion_matrix': []\n",
    "                }\n",
    "                \n",
    "                # Validación cruzada\n",
    "                for fold, (train_idx, val_idx) in enumerate(kf.split(X_entrena_tensor, Y_entrena_tensor)):\n",
    "                    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "                    # Configuración del experimento en wandb (por fold)\n",
    "                    nombreExperimento = f'TFG_MUL_bs({bs})_lr({lr})_hs({hs})_e({e})_fold({fold+1})'\n",
    "                    wandb.init(\n",
    "                        project=\"TFG_MUL_CV_FOLDS\",\n",
    "                        name=nombreExperimento,\n",
    "                        config={\n",
    "                            \"batch_size\": bs,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"hidden_size\": hs,\n",
    "                            \"epochs\": e,\n",
    "                            \"fold\": fold+1\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    # Divisón train/val para este fold\n",
    "                    train_data = Subset(DatasetTFG(X_entrena_tensor, Y_entrena_tensor), train_idx)\n",
    "                    val_data = Subset(DatasetTFG(X_entrena_tensor, Y_entrena_tensor), val_idx)\n",
    "                    \n",
    "                    train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "                    val_loader = DataLoader(val_data, batch_size=bs)\n",
    "                    \n",
    "                    # Modelo y optimizador - Cambiar la capa de salida según el número de clases\n",
    "                    num_classes = len(torch.unique(Y_entrena_tensor))\n",
    "                    modelo = ModeloMulticlase(input_dim=X_entrena_tensor.shape[1], ventanaOculta=hs, numClases=num_classes)\n",
    "                    optimizador = optim.AdamW(modelo.parameters(), lr=lr)\n",
    "                    # Calcular pesos\n",
    "                    class_weights = compute_class_weight(\n",
    "                        'balanced',\n",
    "                        classes=numpy.unique(Y_entrena_tensor.numpy()),\n",
    "                        y=Y_entrena_tensor.numpy()\n",
    "                    )\n",
    "                    weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "                    # Usar en la pérdida\n",
    "                    perdida = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "                    \n",
    "                    # Entrenamiento\n",
    "                    for epoch in range(e):\n",
    "                        modelo.train()\n",
    "                        for batch_X, batch_Y in train_loader:\n",
    "                            optimizador.zero_grad()\n",
    "                            salidas = modelo(batch_X)\n",
    "                            loss = perdida(salidas, batch_Y.long()) \n",
    "                            loss.backward()\n",
    "                            optimizador.step()\n",
    "                    \n",
    "                    # Evaluación en validation fold\n",
    "                    modelo.eval()\n",
    "                    val_preds, val_probs, val_targets = [], [], []\n",
    "                    val_loss = 0.0\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for batch_X_val, batch_Y_val in val_loader:\n",
    "                            salidas_val = modelo(batch_X_val)\n",
    "                            val_loss += perdida(salidas_val, batch_Y_val.long()).item()\n",
    "                            \n",
    "                            # Obtener probabilidades y predicciones\n",
    "                            probs = torch.softmax(salidas_val, dim=1)\n",
    "                            preds = torch.argmax(probs, dim=1)\n",
    "                            \n",
    "                            val_probs.extend(probs.cpu().numpy())\n",
    "                            val_preds.extend(preds.cpu().numpy())\n",
    "                            val_targets.extend(batch_Y_val.cpu().numpy())\n",
    "                    \n",
    "                    # Métricas para este fold\n",
    "                    val_loss /= len(val_loader)\n",
    "                    cm = confusion_matrix(val_targets, val_preds)\n",
    "                    \n",
    "                    # Cálculo de métricas multiclase\n",
    "                    fold_metrics['loss'].append(val_loss)\n",
    "                    fold_metrics['accuracy'].append(accuracy_score(val_targets, val_preds))\n",
    "                    fold_metrics['precision_macro'].append(precision_score(val_targets, val_preds, average='macro'))\n",
    "                    fold_metrics['recall_macro'].append(recall_score(val_targets, val_preds, average='macro'))\n",
    "                    fold_metrics['f1_macro'].append(f1_score(val_targets, val_preds, average='macro'))\n",
    "                    fold_metrics['precision_weighted'].append(precision_score(val_targets, val_preds, average='weighted'))\n",
    "                    fold_metrics['recall_weighted'].append(recall_score(val_targets, val_preds, average='weighted'))\n",
    "                    fold_metrics['f1_weighted'].append(f1_score(val_targets, val_preds, average='weighted'))\n",
    "                    \n",
    "                    # ROC AUC (solo si no es demasiado costoso computacionalmente)\n",
    "                    try:\n",
    "                        fold_metrics['roc_auc_ovo'].append(roc_auc_score(val_targets, val_probs, multi_class='ovo', average='macro'))\n",
    "                        fold_metrics['roc_auc_ovr'].append(roc_auc_score(val_targets, val_probs, multi_class='ovr', average='macro'))\n",
    "                    except:\n",
    "                        fold_metrics['roc_auc_ovo'].append(-1)\n",
    "                        fold_metrics['roc_auc_ovr'].append(-1)\n",
    "                    \n",
    "                    fold_metrics['confusion_matrix'].append(cm)\n",
    "                    \n",
    "                    # Log metrics por fold\n",
    "                    wandb.log({\n",
    "                        \"fold\": fold+1,\n",
    "                        \"loss\": val_loss,\n",
    "                        \"accuracy\": fold_metrics['accuracy'][-1],\n",
    "                        \"precision_macro\": fold_metrics['precision_macro'][-1],\n",
    "                        \"recall_macro\": fold_metrics['recall_macro'][-1],\n",
    "                        \"f1_macro\": fold_metrics['f1_macro'][-1],\n",
    "                        \"precision_weighted\": fold_metrics['precision_weighted'][-1],\n",
    "                        \"recall_weighted\": fold_metrics['recall_weighted'][-1],\n",
    "                        \"f1_weighted\": fold_metrics['f1_weighted'][-1],\n",
    "                        \"roc_auc_ovo\": fold_metrics['roc_auc_ovo'][-1],\n",
    "                        \"roc_auc_ovr\": fold_metrics['roc_auc_ovr'][-1],\n",
    "                        \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                            probs=None,\n",
    "                            y_true=val_targets,\n",
    "                            preds=val_preds,\n",
    "                            class_names=[str(i) for i in range(num_classes)]\n",
    "                        )\n",
    "                    })\n",
    "                    \n",
    "                    wandb.finish()\n",
    "                \n",
    "                # Cálculo de métricas promedio (media de los K folds)\n",
    "                avg_metrics = {k: numpy.mean(v) for k, v in fold_metrics.items() if k != 'confusion_matrix'}\n",
    "                avg_cm = sum(fold_metrics['confusion_matrix']) / len(fold_metrics['confusion_matrix'])\n",
    "                \n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(avg_cm, annot=True, fmt=\".1f\")\n",
    "                plt.title(f\"Avg Confusion Matrix (bs={bs}, lr={lr})\")\n",
    "                \n",
    "                # Log de métricas promedio en wandb (experimento resumen)\n",
    "                wandb.init(\n",
    "                    project=\"TFG_MUL_CV_AVG\",\n",
    "                    name=f\"AVG_bs({bs})_lr({lr})_hs({hs})_e({e})\",\n",
    "                    config={\n",
    "                        \"batch_size\": bs,\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"hidden_size\": hs,\n",
    "                        \"epochs\": e\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                wandb.log({\n",
    "                    \"avg_loss\": avg_metrics['loss'],\n",
    "                    \"avg_accuracy\": avg_metrics['accuracy'],\n",
    "                    \"avg_precision_macro\": avg_metrics['precision_macro'],\n",
    "                    \"avg_recall_macro\": avg_metrics['recall_macro'],\n",
    "                    \"avg_f1_macro\": avg_metrics['f1_macro'],\n",
    "                    \"avg_precision_weighted\": avg_metrics['precision_weighted'],\n",
    "                    \"avg_recall_weighted\": avg_metrics['recall_weighted'],\n",
    "                    \"avg_f1_weighted\": avg_metrics['f1_weighted'],\n",
    "                    \"avg_roc_auc_ovo\": avg_metrics['roc_auc_ovo'],\n",
    "                    \"avg_roc_auc_ovr\": avg_metrics['roc_auc_ovr'],\n",
    "                    \"avg_confusion_matrix\": wandb.Table(\n",
    "                        dataframe=pandas.DataFrame(\n",
    "                            avg_cm,\n",
    "                            index=[f\"True {i}\" for i in range(num_classes)],\n",
    "                            columns=[f\"Pred {i}\" for i in range(num_classes)]\n",
    "                        )\n",
    "                    ),\n",
    "                    \"avg_confusion_matrix_image\": wandb.Image(plt)\n",
    "                })\n",
    "                \n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"\\nResultados promedio para bs={bs}, lr={lr}, hs={hs}, e={e}:\")\n",
    "                print(f\"  Loss: {avg_metrics['loss']:.4f} | Accuracy: {avg_metrics['accuracy']:.4f} | F1 Macro: {avg_metrics['f1_macro']:.4f}\")\n",
    "                \n",
    "                wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c880f5",
   "metadata": {},
   "source": [
    "## Se preparan los datos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f9ce0",
   "metadata": {},
   "source": [
    "X_prueba_normalizado = escalador.transform(X_prueba)\n",
    "\n",
    "X_prueba_tensor = torch.tensor(X_prueba_normalizado, dtype=torch.float32)\n",
    "Y_prueba_tensor = torch.tensor(Y_prueba, dtype=torch.float32)\n",
    "\n",
    "dataset_prueba = DatasetTFG(X_prueba_tensor, Y_prueba_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03803a",
   "metadata": {},
   "source": [
    "## Se guarda el modelo en un fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd254012",
   "metadata": {},
   "source": [
    "torch.save(modelo.state_dict(), 'Modelos/TFG_HUGO_MUL.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61781fd1",
   "metadata": {},
   "source": [
    "### BCEWithLogitsLoss\n",
    "\n",
    "Época: 1, Pérdida: 0.020396\n",
    "Época: 2, Pérdida: 0.009877\n",
    "Época: 3, Pérdida: 0.004355\n",
    "Época: 4, Pérdida: 0.00114\n",
    "Época: 5, Pérdida: 0.002756\n",
    "Época: 6, Pérdida: 0.001659\n",
    "Época: 7, Pérdida: 0.004422\n",
    "Época: 8, Pérdida: 0.000846\n",
    "Época: 9, Pérdida: 0.000706\n",
    "Época: 10, Pérdida: 0.000514\n",
    "\n",
    "Pérdida durante la prueba: 0.0075, Exactitud:  95.90%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
