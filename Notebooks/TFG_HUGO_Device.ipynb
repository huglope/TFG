{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082e6c34",
   "metadata": {},
   "source": [
    "# TFG: Título del TFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08820e3c",
   "metadata": {},
   "source": [
    "## Hugo López Álvarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66de8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy    \n",
    "import pandas   \n",
    "import wandb\n",
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss, fbeta_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6b379",
   "metadata": {},
   "source": [
    "## Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e415a",
   "metadata": {},
   "source": [
    "Definición de la clase DatasetTFG que se usará para entrenar al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTFG(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a33f8e",
   "metadata": {},
   "source": [
    "Definición de la clase Modelo\n",
    "- La capa1 transforma la dimensión de entrada a 64 neuronas\n",
    "- La capa2 pasa de las 64 neuronas a 1 neurona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b046d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelo(nn.Module):\n",
    "    def __init__(self, input_dim, ventanaOculta):\n",
    "        super().__init__()\n",
    "        self.capa1 = nn.Linear(input_dim, ventanaOculta)\n",
    "        self.capa2 =  nn.Linear(ventanaOculta, 1)\n",
    "        self.sigprueba = nn.Sigmoid() #Se pasa el volor por una sigmoidea\n",
    "        \n",
    "    def forward(self,  X):\n",
    "        X = self.capa1(X)\n",
    "        X = self.capa2(X)\n",
    "        #X = self.sigprueba(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240a0da",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_ip_column(df, ip_column_name):\n",
    "    \n",
    "    # Divide la IP en cuatro partes\n",
    "    ip_parts = df[ip_column_name].str.split('.', expand=True)\n",
    "    \n",
    "    # Crea nombres de columnas basados en el nombre original\n",
    "    new_columns = {\n",
    "        0: f\"{ip_column_name}_part1\",\n",
    "        1: f\"{ip_column_name}_part2\", \n",
    "        2: f\"{ip_column_name}_part3\",\n",
    "        3: f\"{ip_column_name}_part4\"\n",
    "    }\n",
    "    \n",
    "    # Se elimina la columna de ip_column_name\n",
    "    df = df.drop(columns=[ip_column_name]) \n",
    "    \n",
    "    # Añade las nuevas columnas al DataFrame\n",
    "    for part, col_name in new_columns.items():\n",
    "        df[col_name] = pandas.to_numeric(ip_parts[part])  # Convierte a numérico\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc3e1",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281eeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileData = pandas.read_csv('../Datasets/modUQ.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16021721",
   "metadata": {},
   "source": [
    "### Comprobación de la obtención correcta del csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77275e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96a971",
   "metadata": {},
   "source": [
    "### Se convierten las columnas no numéricas para poder utilizarlas con pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067862ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileData = split_ip_column(fileData, 'IPV4_SRC_ADDR')\n",
    "#fileData = split_ip_column(fileData, 'IPV4_DST_ADDR')\n",
    "fileData['Attack'] = LabelEncoder().fit_transform(fileData['Attack'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac848f",
   "metadata": {},
   "source": [
    "### Se comprueba que los datos se han transformado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fileData.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28c573",
   "metadata": {},
   "source": [
    "## Se eliminan los datos con valores infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"¿Existen valores infinitos en X?: \", numpy.isinf(fileData.values).any())\n",
    "fileData = fileData.replace([numpy.inf, -numpy.inf], numpy.nan).dropna()\n",
    "#print(\"¿Siguen existiendo valores infinitos en X?: \", numpy.isinf(fileData.values).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1803ca7d",
   "metadata": {},
   "source": [
    "### Se separan las características (X) de la etiqueta (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fileData.drop(columns=['Label', 'Attack', 'FLOW_START_MILLISECONDS', 'FLOW_END_MILLISECONDS', 'IPV4_SRC_ADDR', 'IPV4_DST_ADDR']).values\n",
    "Y = fileData['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f7f18",
   "metadata": {},
   "source": [
    "### Se elimina fileData que contiene el csv con los datoa para liberar memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del fileData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84d514",
   "metadata": {},
   "source": [
    "## Se separan los datos del entrenamiento de los datos de prueba\n",
    "El entrenamiento tendrá el 80% de los datos\n",
    "\n",
    "La prueba tendrá el 20% de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5706da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entrena, X_prueba, Y_entrana, Y_prueba = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42,  stratify=Y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085b6e7",
   "metadata": {},
   "source": [
    "## Se normalizan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ffa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "escalador = MinMaxScaler(feature_range=(0,1))\n",
    "X_entrena_normalizado = escalador.fit_transform(X_entrena)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a8afc",
   "metadata": {},
   "source": [
    "### Se convierten los datos a tensores de Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entrena_tensor = torch.tensor(X_entrena_normalizado, dtype=torch.float32)\n",
    "Y_entrena_tensor = torch.tensor(Y_entrana, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fec3c1",
   "metadata": {},
   "source": [
    "## Creación del Dataset personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_entrena = DatasetTFG(X_entrena_tensor, Y_entrena_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49088650",
   "metadata": {},
   "source": [
    "## Se configura pérdida y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perdida = nn.BCELoss() if len(Y_entrena_tensor.unique()) > 2 else nn.CrossEntropyLoss()\n",
    "pos_weight = torch.tensor([(len(Y_entrana)-sum(Y_entrana))/sum(Y_entrana)])  # Auto-cálculo\n",
    "perdida = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af039dc",
   "metadata": {},
   "source": [
    "## Selección de GPU si existe\n",
    "\n",
    "Los resultados han sido peores de lo esperado utilizando la GPU frente a la CPU\n",
    "![alt text](./imgs/resultadosGPU.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Entrenando en: {device}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4591504a",
   "metadata": {},
   "source": [
    "## Definición de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=[2000, 10000, 15000, 20000]\n",
    "learning_rate=[0.01, 0.001, 0.0001]\n",
    "epochs=[10, 20, 30]\n",
    "hidden_factor=[math.ceil(X_entrena_tensor.size(1)/2), X_entrena_tensor.size(1), X_entrena_tensor.size(1)*2] # la mitad de las columnas, el número de columnas y el doble\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece36861",
   "metadata": {},
   "source": [
    "## Bucle de entrenamiento o épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134de800",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in batch_size:\n",
    "    for lr in learning_rate:\n",
    "        for hs in hidden_factor:\n",
    "            for e in epochs:\n",
    "                # Listas para almacenar métricas de cada fold\n",
    "                fold_metrics = {\n",
    "                    'loss': [], 'accuracy': [], 'precision': [], \n",
    "                    'recall': [], 'f1': [], 'f2': [], 'roc_auc': [],\n",
    "                    'specificity': [], 'tp': [], 'fp': [], 'tn': [], 'fn': []\n",
    "                }\n",
    "                \n",
    "                # Validación cruzada\n",
    "                for fold, (train_idx, val_idx) in enumerate(kf.split(X_entrena_tensor.cpu(), Y_entrena_tensor.cpu())):\n",
    "                    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "                    # Configuración del experimento en wandb (por fold)\n",
    "                    nombreExperimento = f'TFG_BIN_bs({bs})_lr({lr})_hs({hs})_e({e})_fold({fold+1})'\n",
    "                    wandb.init(\n",
    "                        project=\"TFG_BIN_CV_FOLDS\",\n",
    "                        name=nombreExperimento,\n",
    "                        config={\n",
    "                            \"batch_size\": bs,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"hidden_size\": hs,\n",
    "                            \"epochs\": e,\n",
    "                            \"fold\": fold+1,\n",
    "                            \"device\": str(device)\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    # Divisón train/val para este fold\n",
    "                    X_train_fold = X_entrena_tensor[train_idx].to(device)\n",
    "                    Y_train_fold = Y_entrena_tensor[train_idx].to(device)\n",
    "                    X_val_fold = X_entrena_tensor[val_idx].to(device)\n",
    "                    Y_val_fold = Y_entrena_tensor[val_idx].to(device)\n",
    "                    \n",
    "                    train_data = torch.utils.data.TensorDataset(X_train_fold, Y_train_fold)\n",
    "                    val_data = torch.utils.data.TensorDataset(X_val_fold, Y_val_fold)\n",
    "\n",
    "                    train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "                    val_loader = DataLoader(val_data, batch_size=bs)\n",
    "                    \n",
    "                    # Modelo y optimizador\n",
    "                    modelo = Modelo(input_dim=X_entrena_tensor.shape[1], ventanaOculta=hs).to(device)\n",
    "                    optimizador = optim.AdamW(modelo.parameters(), lr=lr)\n",
    "                    perdida = nn.BCEWithLogitsLoss().to(device)\n",
    "                    \n",
    "                    # Entrenamiento\n",
    "                    for epoch in range(e):\n",
    "                        modelo.train()\n",
    "                        for batch_X, batch_Y in train_loader:\n",
    "                            optimizador.zero_grad()\n",
    "                            salidas = modelo(batch_X)\n",
    "                            loss = perdida(salidas, batch_Y.unsqueeze(1))\n",
    "                            loss.backward()\n",
    "                            optimizador.step()\n",
    "                    \n",
    "                    # Evaluación en validation fold\n",
    "                    modelo.eval()\n",
    "                    val_preds, val_probs, val_targets = [], [], []\n",
    "                    val_loss = 0.0\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for batch_X_val, batch_Y_val in val_loader:\n",
    "                            salidas_val = modelo(batch_X_val)\n",
    "                            val_loss += perdida(salidas_val, batch_Y_val.unsqueeze(1)).item()\n",
    "                            probs = torch.sigmoid(salidas_val)\n",
    "                            preds = (probs > 0.5).int()\n",
    "                            val_probs.extend(probs.cpu().numpy())\n",
    "                            val_preds.extend(preds.cpu().numpy())\n",
    "                            val_targets.extend(batch_Y_val.cpu().numpy())\n",
    "                    \n",
    "                    # Métricas para este fold\n",
    "                    val_loss /= len(val_loader)\n",
    "                    tn, fp, fn, tp = confusion_matrix(val_targets, val_preds).ravel()\n",
    "                    \n",
    "                    fold_metrics['loss'].append(val_loss)\n",
    "                    fold_metrics['accuracy'].append(accuracy_score(val_targets, val_preds))\n",
    "                    fold_metrics['precision'].append(precision_score(val_targets, val_preds, zero_division=0))\n",
    "                    fold_metrics['recall'].append(recall_score(val_targets, val_preds, zero_division=0))\n",
    "                    fold_metrics['f1'].append(f1_score(val_targets, val_preds))\n",
    "                    fold_metrics['f2'].append(fbeta_score(val_targets, val_preds, beta=2))\n",
    "                    fold_metrics['roc_auc'].append(roc_auc_score(val_targets, val_probs))\n",
    "                    fold_metrics['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "                    fold_metrics['tp'].append(tp)\n",
    "                    fold_metrics['fp'].append(fp)\n",
    "                    fold_metrics['tn'].append(tn)\n",
    "                    fold_metrics['fn'].append(fn)\n",
    "                    \n",
    "                    # Log metrics por fold\n",
    "                    wandb.log({\n",
    "                        \"fold\": fold+1,\n",
    "                        \"loss\": val_loss,\n",
    "                        \"accuracy\": fold_metrics['accuracy'][-1],\n",
    "                        \"precision\": fold_metrics['precision'][-1],\n",
    "                        \"recall\": fold_metrics['recall'][-1],\n",
    "                        \"f1\": fold_metrics['f1'][-1],\n",
    "                        \"f2\": fold_metrics['f2'][-1],\n",
    "                        \"roc_auc\": fold_metrics['roc_auc'][-1],\n",
    "                        \"specificity\": fold_metrics['specificity'][-1],\n",
    "                        \"true_positives\": tp,\n",
    "                        \"false_positives\": fp,\n",
    "                        \"true_negatives\": tn,\n",
    "                        \"false_negatives\": fn\n",
    "                    })\n",
    "                    \n",
    "                    wandb.finish()\n",
    "                \n",
    "                # Cálculo de métricas promedio (media de los K folds)\n",
    "                avg_metrics = {k: numpy.mean(v) for k, v in fold_metrics.items()}\n",
    "                \n",
    "                # Log de métricas promedio en wandb (experimento resumen)\n",
    "                wandb.init(\n",
    "                    project=\"TFG_BIN_CV_AVG                     \",\n",
    "                    name=f\"AVG_bs({bs})_lr({lr})_hs({hs})_e({e})\",\n",
    "                    config={\n",
    "                        \"batch_size\": bs,\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"hidden_size\": hs,\n",
    "                        \"epochs\": e,\n",
    "                        \"device\": str(device)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                wandb.log({\n",
    "                    \"avg_loss\": avg_metrics['loss'],\n",
    "                    \"avg_accuracy\": avg_metrics['accuracy'],\n",
    "                    \"avg_precision\": avg_metrics['precision'],\n",
    "                    \"avg_recall\": avg_metrics['recall'],\n",
    "                    \"avg_f1\": avg_metrics['f1'],\n",
    "                    \"avg_f2\": avg_metrics['f2'],\n",
    "                    \"avg_roc_auc\": avg_metrics['roc_auc'],\n",
    "                    \"avg_specificity\": avg_metrics['specificity'],\n",
    "                    \"avg_tp\": avg_metrics['tp'],\n",
    "                    \"avg_fp\": avg_metrics['fp'],\n",
    "                    \"avg_tn\": avg_metrics['tn'],\n",
    "                    \"avg_fn\": avg_metrics['fn']\n",
    "                })\n",
    "                \n",
    "                print(f\"\\nResultados promedio para bs={bs}, lr={lr}, hs={hs}, e={e}:\")\n",
    "                print(f\"  Loss: {avg_metrics['loss']:.4f} | Accuracy: {avg_metrics['accuracy']:.4f} | F1: {avg_metrics['f1']:.4f}\")\n",
    "                \n",
    "                wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c880f5",
   "metadata": {},
   "source": [
    "## Se preparan los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prueba_normalizado = escalador.transform(X_prueba)\n",
    "\n",
    "X_prueba_tensor = torch.tensor(X_prueba_normalizado, dtype=torch.float32)\n",
    "Y_prueba_tensor = torch.tensor(Y_prueba, dtype=torch.float32)\n",
    "\n",
    "dataset_prueba = DatasetTFG(X_prueba_tensor, Y_prueba_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03803a",
   "metadata": {},
   "source": [
    "## Se guarda el modelo en un fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd254012",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelo.state_dict(), 'modeloSinLabelNiAttackNiIPSRC.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61781fd1",
   "metadata": {},
   "source": [
    "### BCEWithLogitsLoss\n",
    "\n",
    "Época: 1, Pérdida: 0.020396\n",
    "Época: 2, Pérdida: 0.009877\n",
    "Época: 3, Pérdida: 0.004355\n",
    "Época: 4, Pérdida: 0.00114\n",
    "Época: 5, Pérdida: 0.002756\n",
    "Época: 6, Pérdida: 0.001659\n",
    "Época: 7, Pérdida: 0.004422\n",
    "Época: 8, Pérdida: 0.000846\n",
    "Época: 9, Pérdida: 0.000706\n",
    "Época: 10, Pérdida: 0.000514\n",
    "\n",
    "Pérdida durante la prueba: 0.0075, Exactitud:  95.90%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
